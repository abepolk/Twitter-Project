{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find a way to save the tweet to disk\n",
    "\n",
    "import json\n",
    "import oauth2 as oauth\n",
    "import os\n",
    "import emoji as emo\n",
    "import urllib.parse as parse\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stemming.porter2 import stem\n",
    "from string import punctuation\n",
    "import numpy\n",
    "import pymongo\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function will use an online ready-made dictionary to find the sentiment of an emoji\n",
    "\n",
    "#load csv info into dictionary\n",
    "emoji_scores = {}\n",
    "with open('Emoji_Sentiment_Data_v1.0.csv', newline = '') as emoji_csv:\n",
    "    emoji_reader = csv.reader(emoji_csv)\n",
    "    next(emoji_reader, None)\n",
    "    for row in emoji_reader:\n",
    "        emoji = row[0]\n",
    "        neg_score = float(int(row[4])/int(row[2]))\n",
    "        neut_score = float(int(row[5])/int(row[2]))\n",
    "        pos_score = float(int(row[6])/int(row[2]))\n",
    "        sent_score = (-1 * neg_score + pos_score) - .305\n",
    "        emoji_scores[emoji] = sent_score\n",
    "        \n",
    "#print(emoji_scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Key and secret must be set.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5d8a71b47c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccess_token_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ACCESS_TOKEN_SECRET'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mconsumer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConsumer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconsumer_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconsumer_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0maccess_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccess_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccess_token_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Abe/anaconda3/lib/python3.6/site-packages/oauth2/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, key, secret)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Key and secret must be set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Key and secret must be set."
     ]
    }
   ],
   "source": [
    "consumer_key = os.environ.get('CONSUMER_KEY')\n",
    "consumer_secret = os.environ.get('CONSUMER_SECRET')\n",
    "\n",
    "access_token = os.environ.get('ACCESS_TOKEN')\n",
    "access_token_secret = os.environ.get('ACCESS_TOKEN_SECRET')\n",
    "\n",
    "consumer = oauth.Consumer(key=consumer_key, secret=consumer_secret)\n",
    "access_token = oauth.Token(key=access_token, secret=access_token_secret)\n",
    "client = oauth.Client(consumer, access_token)\n",
    "\n",
    "\n",
    "\n",
    "emo_tweets = {}\n",
    "for i in range(0, 10):\n",
    "    params = {'q' : 'Justin Bieber', 'count' : 100, 'lang' : 'en'}\n",
    "    # See Twitter Search API documenation for description of cursoring for below if block\n",
    "    if i > 0:\n",
    "        params['max_id'] = max_id \n",
    "    timeline_endpoint_baseuri = 'https://api.twitter.com/1.1/search/tweets.json?'\n",
    "    timeline_endpoint = timeline_endpoint_baseuri + parse.urlencode(params)\n",
    "    print(timeline_endpoint)\n",
    "    response, data = client.request(timeline_endpoint)\n",
    "    tweets_results = json.loads(data)\n",
    "    tweets_list = tweets_results['statuses']\n",
    "    max_id = tweets_list[len(tweets_list) - 1]['id'] - 1\n",
    "    for tweet in tweets_list:\n",
    "        tweet_emojis = []\n",
    "        for char in tweet['text']:\n",
    "            if char in emo.UNICODE_EMOJI:\n",
    "                tweet_emojis.append(char)\n",
    "        if tweet_emojis:\n",
    "            for emoji in tweet_emojis:\n",
    "                tweet_emoji_scores = []\n",
    "                try:\n",
    "                    tweet_emoji_scores.append(emoji_scores[emoji])\n",
    "                except KeyError as e:\n",
    "                    print('No emoji %s' % e)\n",
    "                    continue\n",
    "            if tweet_emoji_scores:\n",
    "                if max(tweet_emoji_scores) == max(numpy.absolute(tweet_emoji_scores)):\n",
    "                    tweet_score = max(tweet_emoji_scores)\n",
    "                else:\n",
    "                    tweet_score = min(tweet_emoji_scores)\n",
    "                emo_tweets[tweet['id']] = [tweet['text'], tweet_score]\n",
    "    print('Emoji tweets processed: ' + str(len(emo_tweets.keys())))\n",
    "            \n",
    "\n",
    "for tweet in emo_tweets.values():\n",
    "    print(tweet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Been You - Justin Bieber üé§', 0.18111111111111106], ['RT @ThaBieberCrew: Do you remember searching \"Justin Bieber\" on Google and this came up? üòç #MTVHottest Justin Bieber https://t.co/mhe7uYbsko', 0.37293678251297374], ['RT @sheEzo_66: We were born for this üé∂‚≠ê\\n#MTVHottest Justin Bieber', 0.23422967189728955], ['RT @sheEzo_66: We were born for this üé∂‚≠ê\\n#MTVHottest Justin Bieber', 0.2861949685534591], ['RT @sharon_arcadio: I love this picture‚ù§Ô∏è #MTVHottest Justin Bieber https://t.co/pfXVbhyc8H', 0.4410869565217392], ['RT @godisbiebers: Hey puppy @justinbieber hey beliebers üíï\\n#MTVHottest Justin Bieber', 0.3279166666666667], ['RT @GayTimesMag: Justin Bieber slowly undresses in a new video that will leave you swooning hard üòç\\n\\nhttps://t.co/cYkW1i9P5R https://t.co/mS‚Ä¶', 0.37293678251297374], ['RT @ginnyweasleys: Justin Bieber as albus dumbledore (ripüòî‚ù§Ô∏è) https://t.co/1bPGZF9FNw', -0.4510580912863071], ['RT @ginnyweasleys: Justin Bieber as albus dumbledore (ripüòî‚ù§Ô∏è) https://t.co/1bPGZF9FNw', 0.4410869565217392], [\"RT @nutofbieber: this will be the young justin bieber we'll look back to in a few years üòç https://t.co/x2cdEDxDbn\", 0.37293678251297374], ['‚Ä¢‚Ä¢‚Ä¢ pack; Justin Bieber lockscreens + homescreens ‚Ä¢‚Ä¢‚Ä¢\\n\\nüìå mbf\\nüìå rt if saved\\nüìå give credits if used\\n\\n‚Ä¢‚Ä¢ pp üåª https://t.co/JBH9OFDILp', -0.8764285714285713], ['‚Ä¢‚Ä¢‚Ä¢ pack; Justin Bieber lockscreens + homescreens ‚Ä¢‚Ä¢‚Ä¢\\n\\nüìå mbf\\nüìå rt if saved\\nüìå give credits if used\\n\\n‚Ä¢‚Ä¢ pp üåª https://t.co/JBH9OFDILp', -0.8764285714285713], ['‚Ä¢‚Ä¢‚Ä¢ pack; Justin Bieber lockscreens + homescreens ‚Ä¢‚Ä¢‚Ä¢\\n\\nüìå mbf\\nüìå rt if saved\\nüìå give credits if used\\n\\n‚Ä¢‚Ä¢ pp üåª https://t.co/JBH9OFDILp', -0.8764285714285713], ['‚Ä¢‚Ä¢‚Ä¢ pack; Justin Bieber lockscreens + homescreens ‚Ä¢‚Ä¢‚Ä¢\\n\\nüìå mbf\\nüìå rt if saved\\nüìå give credits if used\\n\\n‚Ä¢‚Ä¢ pp üåª https://t.co/JBH9OFDILp', 0.28153846153846157], ['RT @JBBIZZLS: Goodmorning my big family ‚ù§Ô∏è \\n#MTVHottest Justin Bieber https://t.co/kblQGEPVUy', 0.4410869565217392]]\n"
     ]
    }
   ],
   "source": [
    "# text preprocessing (partially from before partially from August)\n",
    "mongo_uri = 'mongodb://heroku_xgnhblcr:' + os.environ.get('MONGODB_PASSWORD') + '@ds149511.mlab.com:49511/heroku_xgnhblcr'\n",
    "client = pymongo.MongoClient(mongo_uri)\n",
    "db = client.get_default_database()\n",
    "myresults = list(db.Justin_Bieber.find())\n",
    "tweets = []\n",
    "emoji_tweets = []\n",
    "for entry in myresults:\n",
    "    tweets.append(entry['text'])\n",
    "#Doing the next part based on the earlier REST-API-using cell\n",
    "for tweet in tweets:\n",
    "    tweet_emojis = []\n",
    "    for char in tweet:\n",
    "        if char in emo.UNICODE_EMOJI:\n",
    "            tweet_emojis.append(char)\n",
    "    if tweet_emojis:\n",
    "        for emoji in tweet_emojis:\n",
    "            tweet_emoji_scores = []\n",
    "            try:\n",
    "                tweet_emoji_scores.append(emoji_scores[emoji])\n",
    "            except KeyError as e:\n",
    "                print('No emoji score for %s' % e)\n",
    "                continue\n",
    "            if tweet_emoji_scores:\n",
    "                if max(tweet_emoji_scores) == max(numpy.absolute(tweet_emoji_scores)):\n",
    "                    tweet_score = max(tweet_emoji_scores)\n",
    "                else:\n",
    "                    tweet_score = min(tweet_emoji_scores)\n",
    "                emoji_tweets.append([tweet, tweet_score])\n",
    "    \n",
    "print(emoji_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_text= []\n",
    "for tweet in emoji_tweets:\n",
    "    tweet_text.append(tweet[0])\n",
    "    \n",
    "    \n",
    "def preprocessor(tweet):\n",
    "    \n",
    "    #remove accents\n",
    "    normalized = unicodedata.normalize('NFKD', tweet)\n",
    "    if normalized != tweet:\n",
    "        tweet = ''.join([c for c in normalized if not unicodedata.combining(c)])\n",
    "        \n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #remove URLs\n",
    "    tweet = re.sub(r'https?:\\/\\/.*?(\\s|$)', '', tweet)\n",
    "    tweet = re.sub(r'@\\w*?(\\s|$)', '', tweet)\n",
    "    tweet = re.sub(r'#\\w*?(\\s|$)', '', tweet)\n",
    "    \n",
    "    #remove RT (retweet) - this doesn't seem to work since 'rt' is in the preprocessed data\n",
    "    tweet = re.sub('^rt\\s', '', tweet)\n",
    "    \n",
    "    #Note - the preprocessor already seems to be removing emojis - no need for extra code\n",
    "    \n",
    "    # Regularize apostrophes from different parts of unicode\n",
    "    tweet = re.sub('|'.join(['‚Äò', '‚Äô', ' ª', ' º']), '\\'', tweet)\n",
    "    \n",
    "    return tweet\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hard', \"I'll\", 'as', 'respect', \"y'all\", 'Barack', 'none', 'https', 't']\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(tweet): #Currently troubleshooting function\n",
    "    \n",
    "    tokens = []\n",
    "    token_pattern = '(^|(?!\\w).)((\\w|\\')+)\\W'\n",
    "    token_matches = re.finditer(token_pattern, tweet)\n",
    "    for token_match in token_matches:\n",
    "        tokens.append(token_match.group(2))\n",
    "    return tokens\n",
    "\n",
    "print(tokenizer(\"Hard pass...I'll give as much respect as y'all did Barack...none üòí https://t.co/UJcr4mrZRA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'as', 'back', 'be', 'been', 'bieber', 'big', 'born', 'came', 'do', 'dumbledore', 'few', 'gaytimesmag', 'ginnyweasleys', 'give', 'godisbiebers', 'goodmorning', 'hey', 'homescreens', 'i', 'if', 'in', 'jbbizzls', 'justin', 'leave', 'll', 'lockscreens', 'mbf', 'new', 'nutofbieber', 'on', 'pack', 'pp', 'remember', 'rip', 'rt', 'saved', 'sharon_arcadio', 'sheezo_66', 'slowly', 'swooning', 'thabiebercrew', 'that', 'this', 'we', 'young']\n",
      "  (0, 23)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 41)\t1\n",
      "  (1, 23)\t2\n",
      "  (2, 43)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 44)\t1\n",
      "  (2, 38)\t1\n",
      "  (3, 43)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 44)\t1\n",
      "  (3, 38)\t1\n",
      "  (4, 19)\t1\n",
      "  (4, 37)\t1\n",
      "  (4, 43)\t1\n",
      "  (4, 23)\t1\n",
      "  (5, 17)\t2\n",
      "  (5, 15)\t1\n",
      "  (6, 40)\t1\n",
      "  (6, 24)\t1\n",
      "  :\t:\n",
      "  (11, 23)\t1\n",
      "  (12, 32)\t1\n",
      "  (12, 20)\t1\n",
      "  (12, 14)\t1\n",
      "  (12, 36)\t1\n",
      "  (12, 35)\t1\n",
      "  (12, 27)\t1\n",
      "  (12, 18)\t1\n",
      "  (12, 26)\t1\n",
      "  (12, 31)\t1\n",
      "  (12, 23)\t1\n",
      "  (13, 32)\t1\n",
      "  (13, 20)\t1\n",
      "  (13, 14)\t1\n",
      "  (13, 36)\t1\n",
      "  (13, 35)\t1\n",
      "  (13, 27)\t1\n",
      "  (13, 18)\t1\n",
      "  (13, 26)\t1\n",
      "  (13, 31)\t1\n",
      "  (13, 23)\t1\n",
      "  (14, 6)\t1\n",
      "  (14, 16)\t1\n",
      "  (14, 22)\t1\n",
      "  (14, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(preprocessor=preprocessor, tokenizer=tokenizer) #Make binary?  Does this prevent it from picking up one word twice in a tweet?\n",
    "fit_tr = vectorizer.fit_transform(tweet_text)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(fit_tr)\n",
    "#   for word in emoji_tweets.split(): #This and following line should be removed and use regex instead.  Follow Downloads/pak-paroubek.pdf for preprocessing ideas, as well as downloads that were downloaded at a similar time\n",
    "#      if word[0:4] == 'http':\n",
    "            \n",
    "#CountVectorizer().build_tokenizer()(tweet_text) #stop_words param to 'english' once you have only Eng tweets, this was indented within for loop at the beginning of August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()  # This block is not correct\n",
    "tfidf = transformer.fit_transform(fit_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[ 0.60759494  0.64556962  0.82051282  0.74025974  0.67532468]\n"
     ]
    }
   ],
   "source": [
    "    tweet_emos = []\n",
    "    for tweet in emoji_tweets:\n",
    "        tweet_emos.append(tweet[1] > 0)\n",
    "        \n",
    "    \n",
    "    #Cross-validation\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    print(type(tfidf.toarray()))\n",
    "          \n",
    "    scores = cross_val_score(MultinomialNB(alpha=.01), tfidf, tweet_emos, cv=5)\n",
    "    print(scores)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    MultinomialNB(alpha=.01).fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
